<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
</head>

<body>

<d-front-matter>
  <script type="text/json">{
  "title": "Differentiable Image Parameterizations",
  "description": "Examples of differentiable image parameterizations that allow solving previously intractable optimization tasks.",
  "password": "params",
  "authors": [
    {
      "author": "Alexander Mordvintsev",
      "authorURL": "https://znah.net/",
      "affiliation": "Google Research",
      "affiliationURL": "https://research.google.com/"
    }
  ],
  "katex": {
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
</d-front-matter>

<d-title>
  <h1>Differentiable Image Parameterizations</h1>
</d-title>

<d-article>
  
  <nav>
    <!-- This TOC is for development and will be removed before publication. -->
    <style>
      d-toc#toc-dev-only ul {
        padding-left: 24px;
      }
      d-toc#toc-dev-only li {
        margin-bottom: -1.7em;
      }
    </style>
    <d-toc id='toc-dev-only'></d-toc>
  </nav>

  <p>
    This cites our <d-cite key="olah2017feature">Feature visualization</d-cite> article.
  </p>
  
  <h2 id='experiment-aligned'>Aligned Neuron Interpolation</h2>
  
  <p>
    As an introductory example, we explore how <a href="https://distill.pub/2017/feature-visualization/#interpolation">aligned interpolation neuron visualizations</a> are created.
    The motivation for a non-standard image parameterization comes from the desire to have the interpolation be visually aligned, as in the following figure originally published in <d-cite key="olah2017feature">Feature Visualization</d-cite>:
  </p>
  <d-figure class="base-grid" id='AlignedInterpolationExamples'></d-figure>
  <p>
    To see the problem with a naive approach, compare the following animations.
    In the first animation, each image was optimized independently for a convex combination of the two neurons. The resulting animation is jumpy. In the second animation we used a shared parameterization we will detail shortly. Notice how much smoother the second animation appears: (TODO: hover to play indicator, layout)
  </p>
  <d-figure id='AlignedInterpolationAnimations'></d-figure>
  <p>
    To achieve this visual alignment we optimized the frames of the second animation as a combination of a lower-resolution shared parameterization and a higher resolution parameterization unique to each frame.
    Thus the parameterization $P^i$ of each image $i$ is $ N(P_{\text{unique}}^i + P_{\text{shared}}) $ where $N$ is usually the logistic function.
  </p>
  <div>
    <%= require('!svg-inline-loader!../static/images/lowres-tensor.svg') %>
  </div>
  
  <h2 id='experiment-styletransfer'>Style Transfer on non-VGG architectures</h2>
  
  <p>
    Explanation
  </p>
  
  <d-figure class="base-grid" id='StyleTransferExamples'></d-figure>
  
  <h2 id='experiment-featureviz-rgba'>Feature Visualization with Transparency</h2>
  

  
  <h2 id='experiment-featureviz-3d'>Efficient Texture Optimization through 3D Rendering</h2>
  
  <p>  
So far we have been focusing only on 2D images, as they are the natural domain for the most common Deep Neural Networks models. 
Would it be possible to extend the generative capabilities that we have demonstrated in the previous sections also for the creation of artistic 3D objects without building ad-hoc networks [voxel-dnn]? In this section we present a generative approach that make use of the 3D rendering pipeline combined with the generative capabilities of neural networks. A few examples of the results are shown in the picture above.
  </p>
  </p>
Before we can describe our approach, we first need to understand how a 3D object is stored and rendered on screen. The geometry of the object is usually represented as a collection of interconnected triangles which we refer as a triangular mesh or, simply, mesh. In order to generate a realistic model, we need to add a texture over the triangular mesh. The texture is saved as an image that is then applied to the model using the so called UV-mapping. Every vertex $c_i$ in the mesh is associated to a $(u_i,v_i)$ coordinate in the texture image. The model is then rendered, i.e. drawn on screen, by coloring every triangle with the region of the image that is delimited by the $(u,v)$ coordinates of its vertices.
  </p>
  </p>
You can use the following WebGL [webgl] view to familiarize with these concepts. The view shows the 3D model of the famous Stanford Bunny []. You can interact with the model by rotating, zooming and panning. Moreover, you can unfold the object to its two-dimensional texture representation. This unfolding reveals the UV mapping used to store the texture in the two dimensional image. Note how, for complex geometries, the texture is divided in several patches that allows for a complete and non-distorted coverage of the object.
  </p>
  <d-figure class="base-grid" id="3DStyleTransferExamples"></d-figure>
    </p>
Our goal is to generate a texture by visualizing a combination of features learned by a network trained for an object recognition task [featureviz]. A simple and straightforward way to achieve this result is to use, as texture to color the object, an image that is obtained by optimizing for one or more features in a layer [feature-viz]. The resulting image is then used as texture for the object.
</p>



  <p>
    By interacting with the model, you may have noticed that the result is not optimal and there are several problems that reduce the visual quality of the rendered object. 
First, seams are clearly visible on the rendered texture [footnote: The seem on the snout is particularly evident (maybe ad images of examples)], as the optimization is not aware of the underling UV-mapping and, therefore, does not optimize the texture consistently along the splitted patches of the texture.

  </p>
  <p>
Second, the generated patterns are randomly oriented on different parts of the textured object.
Convolutional networks, like the one used in this experiment [googlelenet], are known to have poor rotation-invariance [reference]. While this is not a problem per-se, as orientation usually matters in a object recognition problem, it does not work well if the texture patches are not consistently oriented in the underlying UV-mapping.

  </p>
  <p>
All the above mentioned problems arise from the fact that the optimization of the texture does not consider the underlying UV-mapping, which in the end is responsible for the appearance of the textured object.
Here we propose a different approach that overcomes this limitation through an optimization that is driven by the images generated by the renderer and, therefore, are closer to what a user will sees.
  </p>
  <p>
We start the process by randomly initializing the texture $T$ as before.
At every training iteration we sample a random camera position $p$, which is oriented towards the center of mass of the object, and we render the textured object as an image $R_p$.
We then backpropagate the gradient of the desired function, i.e. the feature of interest, to $R_p$.
However, an update of $R_p$  does not correspond to an update to the texture that we aim at optimizing. Hence, we need to propagate the changes to $R_p$ to the texture $T$.
The propagation is easily implemented by applying a reverse UV-mapping, as for each pixel on screen we know its coordinate in the texture $T$.
By modifying $T$, during the following optimization iterations, the rendered image $R_p$ will incorporate changes in the texture performed in the previous iterations.
  </p>  
  <p>  
Compared to existing methods [], our approach disentangle the generation of the texture from the 3D position of the mesh.
This characteristic allows for the generation of very detailed texture for very detailed meshes.
The only additional step compared to a traditional backpropagation pipeline, consists in the rendering of the image $R_p$, which is efficiently computed on the GPU using OpenGL [].
  </p>  

  <h2 id='experiment-featureviz-3d'>Style Transfer for Textures through 3D Rendering</h2>

  <p>  
Once that we have a framework for the efficient backpropagation of the gradients in the UV-mapped texture, we may adapt existing style transfer techniques for 3D objects.
In this experiment we need an existing texture for the object that we want to repaint with a different style. 
Similarly to the 2D case, we refer to  the original texture as the content, while the target style is provided as an image[footnote: few of the most used style comprises Van Gogh’s Starry Night, Kandinsky ….].
  </p> 
  
    <p>  
The algorithm works in similar way to the one presented in the previous section, starting from a randomly initialized texture $T$.
At each iteration, we sample a random view point $p$ oriented toward the center of mass of the object. Then, we render an image $C_p$ showing the object with the original texture. Similarly, we render an image $R_p$ where the object is drawn with the texture $T$. 
$C_p$ represents the content that we want to preserve in the image $R_p$, while the desired style is simply given by the style image provided as input.
Once that these three elements are computed, we optimize for the style function introduced by Gatys et al.[1] and we map the parameterization back in the UV-mapped texture as introduced in the previous section.
The procedure is then iterated until the desired blend of content and style is obtained in the target texture $T$.

  </p>   

  <h2 id='experiment-xy2rgb'>xy2rgb, a CPPN</h2>

</d-article>



<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
    TODO: Thank all the people who have contributed somehow.
  </p>

  <h3>Author Contributions</h3>
  <p>
    TODO: List in detail who did what.
  </p>

  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
