<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
</head>

<body>

<d-front-matter>
  <script type="text/json">{
  "title": "Differentiable Image Parameterizations",
  "description": "Examples of differentiable image parameterizations that allow solving previously intractable optimization tasks.",
  "password": "params",
  "authors": [
    {
      "author": "Alexander Mordvintsev",
      "authorURL": "https://znah.net/",
      "affiliation": "Google Research",
      "affiliationURL": "https://research.google.com/"
    }
  ],
  "katex": {
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
</d-front-matter>

<d-title>
  <h1>Differentiable Image Parameterizations</h1>
  <p>Six examples of differentiable image parameterizations that allow solving previously intractable optimization tasks:</p>
  <style>
    .visual-toc {
      counter-reset: toc-heading;
      display: grid;
      grid-auto-flow: dense;
      grid-template-columns: 1fr 1fr 1fr;
      grid-gap: 16px;
      cursor: pointer;
    }
    @media (min-width: 1024px) {
      .visual-toc {
        grid-gap: 20px;
        grid-template-columns: 1fr 1fr 1fr 1fr 1fr 1fr;
      }
    }
    .visual-toc-item {
      border: 1px solid #E5E5E5;
      border-radius: 5px;
      padding-bottom: 1em;
      overflow: hidden;
      text-decoration: none;
      box-shadow: 0px 1px 5px rgba(0,0,0,0.05);
      transition: box-shadow 0.35s, transform 0.35s;
      transform: scale(1.0);
    }
    .visual-toc-item:hover {
      box-shadow: 0px 1px 10px rgba(0,0,0,0.15);
      transform: scale(1.02);
      transition: box-shadow 0.15s, transform 0.15s;
    }
    .visual-toc-img {
      width: 100%;
    }
    .visual-toc-heading,
    .visual-toc-subheading {
      display: block;
      line-height: 1.3em;
      font-size: 85%;
      padding: 0.5em 1em 0 1em;
    }
    .visual-toc-heading {
      counter-increment: toc-heading;
      color: #333;
      font-weight: 600;
      /* text-transform: uppercase; */
    }
    .visual-toc-heading::before{
      display: block;
      content: "Section " counter(toc-heading);
      font-weight: 400;
      text-transform: uppercase;
      font-size: 0.6rem;
      color: #666;
    }
    .visual-toc-subheading {
      display: none;
      color: #666;
      font-size: 75%;
    }
  </style>
  <nav class="l-page visual-toc">
    <a href="#experiment-aligned" class="visual-toc-item">
      <img class="visual-toc-img" src="images/vtoc-aligned-interpolation.png"/>
      <span class="visual-toc-heading">Aligned Neuron Interpolation</span>
      <span class="visual-toc-subheading">by using a shared parameterization</span>
    </a>
    <a href="#experiment-styletransfer" class="visual-toc-item">
      <img class="visual-toc-img" src="images/vtoc-styletransfer.png"/>
      <span class="visual-toc-heading">Style Transfer in the Fourier basis</span>
      <span class="visual-toc-subheading">generalized to non-VGG architectures</span>
    </a>
    <a href="#experiment-featureviz-rgba" class="visual-toc-item">
      <img class="visual-toc-img" src="images/vtoc-alpha.png"/>
      <span class="visual-toc-heading">Feature Vis with transparency</span>
      <span class="visual-toc-subheading">for activation maximization</span>
    </a>
    <a href="#experiment-xy2rgb" class="visual-toc-item">
      <img class="visual-toc-img" src="images/vtoc-cppn.png"/>
      <span class="visual-toc-heading">Feature Vis with CPPNs</span>
      <span class="visual-toc-subheading">for activation maximization</span>
    </a>
    <a href="#experiment-featureviz-3d" class="visual-toc-item">
      <img class="visual-toc-img" src="images/vtoc-3d-opt.png"/>
      <span class="visual-toc-heading">Feature Vis in 3D</span>
      <span class="visual-toc-subheading">for activation maximization</span>
    </a>
    <a href="#experiment-styletransfer-3d" class="visual-toc-item">
      <img class="visual-toc-img" src="images/vtoc-3d-style.png"/>
      <span class="visual-toc-heading">Style Transfer in 3D</span>
      <span class="visual-toc-subheading">Aligned Neuron Interpolation</span>
    </a>
  </nav>
</d-title>

<d-article>

  <!-- <h2 id='introduction'>Introduction</h2> -->
  <p>
Deep Neural Networks are, essentially, very entangled non-linear functions<d-cite key="lecun2015deepLearning"></d-cite>. Despite being highly non-linear, they remain differentiable functions for which the computation of partial derivatives is not only possible, but very efficient on parallel architectures<d-cite key="krizhevsky2012"></d-cite>. The computation of the derivatives with respect to the parameters enables the learning of a desired function. Network's parameters, also called weights, are modified by a tiny amount along the gradient until, after a number of iterations, the output of the network and the desired function match.
  </p>

  <p>
However, other differentiable functions can be propagated, either from the output or the activations of hidden layers, without changing the parameters of the network but rather its input<d-cite key="zeiler2014"></d-cite>.
We will refer to the space in which the input is optimized as its <b>Differentiable Image Parameterization</b>.
The most common and simple parameterization is the image pixel space, where the changes propagated troughout the network correspond to changes in the color of each pixel.
  </p>

  <img style="width: 100%" src="images/diagrams/general.png"/>

  <p>
Depending on which function is propagated, the parameterization will map some network's latent state to a 2D raster image.
For example, DeepDream<d-cite key="mordvintsev2015inceptionism"></d-cite> and <b>feature visualization</b><d-cite key="olah2017feature"></d-cite> objective functions can be used to map in the chosen parameterization the visual patterns that the network learned to detect.
By optimizing for different objective functions, <b>artistic image style transfer</b> can be implemented.
This application was pioneered by Gatys et al. <d-cite key="gatys2015"></d-cite> and is one of the most widespread applications of neural networks for the generation of artistic contents.
The resulting parameterization creates a raster image that preserves the content of a desired image and the style of a different one.
  </p>

  <p>
In this article we explore some more exotic image parameterization and optimization functions that allows for the creation of artistic results. The proposed techniques achieve generative capabilities through an iterative process that makes small adjustments to randomly initialized parameterizations.
Hence, we do not require the creation of ad-hoc feed-forward networks<d-cite key="li2017universal"></d-cite> that learn to emulate the iterative process or the implementation of generative models, such as Generative Adversarial Networks (GANs)<d-cite key="goodfellow2014:adversarial"></d-cite> or Variational Autoencoders (VAEs)<d-cite key="Diederik2013:VAE"></d-cite>.
  </p>

  <p>
We propose different parameterizations for images and extensions that allow for the creation of artistic 3D models.
We believe that our examples will inspire researchers and computational artists in devising novel and exciting computational art.
To further facilitate the development of novel parameterizations, we release the code of our experiments as <a href="https://colab.research.google.com/">CoLab</a> notebooks.
  </p>

<!-- =================================================== -->

  <h2 id='experiment-aligned'>Aligned Neuron Interpolation</h2>

  <p>
As an introductory example, we explore how <a href="https://distill.pub/2017/feature-visualization/#interpolation">aligned interpolation neuron visualizations</a> are created.
Throughout this article we will often optimize a randomly initialized parameterization to generate the patterns that are detected by a neurons, channels or layers in a neural network.
We refer to the result of this procedure as the optimization for a feature visualizations objective function<d-cite key="olah2017feature"></d-cite>, as it reveals the features that the network is detecting in different layers.
  </p>

  <p>
Images can be optimized not only for a single feature, but for a <a href="https://en.wikipedia.org/wiki/Convex_combination">convex combination</a> of two or more features.
By gradually changing the interpolation weight given to one neuron from the other, we can generate compelling animations that shows how a pattern non-linearly blend into another.
  </p>
  <d-figure class="base-grid" id='AlignedInterpolationExamples'></d-figure>

  <p>
The above results enforces an underlying common parameterization for all the intermediate results that lead to the creation of similar image locations for similar patterns.
If the common parameterization is not enforced, patterns would emerge at different locations in every frame.
  </p>

  <p>
To see the problem with an independent optimization, compare the following animations.
For the animation on the left, every frame is independently optimized from the others, resulting in an inconsistent visual appeal. The animation on the right is generated by optimizing each frame with a shared parameterization that we will detail shortly.
The result does not only reveal the transition from one pattern to another, but we believe it has also artistic merits.
  </p>
  <d-figure id='AlignedInterpolationAnimations'></d-figure>
  <p>
Visual alignment is obtained by combining a low-resolution shared parameterization $ P_{\text{shared}}$ and high-resolution parameterization $P_{\text{unique}}^i$ that is unique to each frame $i$.
Each individual frame $i$ of the animation is then parameterized as a combination $P^i$ of the two, $P^i = N(P_{\text{unique}}^i + P_{\text{shared}})$, where $N$ is usually the logistic function.
  </p>

  <div>
    <%= require('!svg-inline-loader!../static/images/lowres-tensor.svg') %>
  </div>

  <p>
    Intuitively, the shared parameterization provides a common reference for the displacement of major patterns, while the unique parameterizations give to each frame its own visual appeal based on its interpolation weights.
  </p>

  <p>
This first example shows a first glimpse of the creative power of custom and properly designed image parameterizations.
In a way, we may think as parameterizations as the clay that a artist can use to sculpt computational art.
  </p>

<!-- =================================================== -->

  <h2 id='experiment-styletransfer'>Good parameterizations generalize style transfer for non-VGG networks
</h2>

  <p>
Neural artistic style transfer<d-cite key="gatys2015"></d-cite> is one of the first and, by far, one of the most widespread application of generative artistic capabilities of convolutional neural networks.
Almost all iterative style transfer implementations utilize a variant of the <b>VGG neural network</b> architecture<d-cite key="simonyan2014vgg"></d-cite> while attempts at using different architectures seem to produce less visually appealing results
<d-footnote>
Examples of experiments performed by using different architectures can be find on <a href="https://medium.com/mlreview/getting-inception-architectures-to-work-with-style-transfer-767d53475bf8">Medium</a>, <a href="https://www.reddit.com/r/MachineLearning/comments/7rrrk3/d_eat_your_vggtables_or_why_does_neural_style/">Reddit</a> and <a href="https://twitter.com/hardmaru/status/954173051330904065">Twitter</a>.
</d-footnote>.
  </p>

  <p>
Various hypothesis have been made to explain the difference in performance between VGG and other models.
A possible explanation is that, being the VGG architecture bigger than others, it manages to capture more information that is not related to the task at hand and that is discarded by other models.
While this information does not give, for example, greater classification accuracy, it improves the generalization of the VGG architecture to a task like artistic style transfer.
Another possible reason is that other models perform more aggressive downsampling, in the form of max-pooling layers, compared to the VGG network.
  </p>

  <p>
Here we demonstrate that the parameterization used to represent the image is as important, if not more important, than the model chosen.
We show that artistic style transfer images produced by the GoogLeNet<d-cite key="szegedy2015googlenet"></d-cite> architecture can be as good as those produced by the VGG network if the right parameterization is chosen.
  </p>

<d-figure class="base-grid" id='StyleTransferExamples'></d-figure>

  <p>
Previous work on feature visualization<d-cite key="olah2017feature"></d-cite> showed that adopting a different parameterization works as preconditioner for the optimization, making it more robust to local minima.
Olah et al.<d-cite key="olah2017feature"></d-cite> showed that high frequency artifacts in optimization results are avoided by working in a decorrelated and whitened space and by enforcing geometric robustness
<d-footnote>
Geometric robustness is achieved by stochastically jitter, rotate and scale the image before applying the optimization step.
</d-footnote>.
  </p>
  <p>
For images, this means that the gradient descent must not be performed in the pixel space, but rather in the space of the Fourier basis where frequencies are scaled so that they all have equal energy. By adopting this parameterization, the style transfer loss function proposed by Gatys<d-cite key="gatys2015"></d-cite> produces convincing results like the ones presented above for non-VGG architectures.
  </p>

  <p>
Before we proceed in developing more exotic parameterizations, it is good to reflect on what we discussed so far.
Differentiable image parameterizations are a key aspect of optimization techniques for artistic content generation.
Underestimating the role of a properly chosen parameterization makes the difference between a poor artistic artifact and a good one.
  </p>

<!-- =================================================== -->

  <h2 id='experiment-featureviz-rgba'>Generation of Semi-Transparent Patterns</h2>
  <p>
Most of existing optimization techniques work on an input-space that has the same format as the input used to originally train the network.
For networks trained to process images, the input usually corresponds to a 2D array, where each pixel is represented by RGB channel values.
In this section we investigate how to extend differentiable optimizations for channels that were not used to train the network. More specifically, we aim at creating semi-transparent images by optimizing the alpha channel of an image in combination with its RGB channels.
  </p>

  <p>
Pixels in the <b>alpha channel</b> are assumed to have values ranging from 0, when they are fully transparent, to 1 if they are completely opaque.
Our goal is to reveal one or more features detected by the network<d-cite key="olah2017feature"></d-cite> while imposing a desired amount of transparency in the generated image.
To this end, we optimize for the feature to be visualized in the RGB image multiplied by its alpha channel and, at the same time, we minimize the squared loss of the average alpha value to a desired threshold $\alpha_t$.

  </p>

<figure class="base-grid shaded-figure">
  <figcaption style="grid-column: kicker;">
    The image is parameterized as the combination of the RGB and alpha channel.
  </figcaption>
  <d-figure style="grid-column: text-start / screen-end; overflow: visible; contain: unset;" id="SemiTransparentCombination"></d-figure>
</figure>

  <p>
By multiplying the RGB channels with the alpha channel, we are de-facto imposing a black background for transparent areas.
Therefore, if the target feature detects black or dark colors, the optimization has two ways for optimizing the result.
It may either generate a dark color in the RGB channels or reduce the transparency accordingly.
  </p>
  <p>
Since we constraint the level of transparency, given by the $\alpha_t$ threshold, the optimization is incentivized to apply the latter of the two strategies.
Unfortunately, this will lead to missing colors in the RGB channels.
The same problem holds if different static colors are chosen as background.
The optimization is never incentivized to propagate the chosen color to the generated image.
  </p>

  <p>
To solve this problem, we randomly change every background pixel at each iteration of the optimization.
In this way, we force the optimization to propagate the colors to the RGB channels and do not "cheat" by making use of a static background.
  </p>
  <p>
We experimented with two different strategies for generating the background.
The first approach rely on a randomly generated background for each pixel, while the second one uses a user provided image which is randomly cropped and used as background.
For the latter, we found that the chosen image must have a diverse palette or the same problem may arise.
  </p>

 <d-figure class="base-grid" id='SemiTransparentExamples'></d-figure>

  <p>
In this section we have shown how we can extend optimization techniques beyond the input space that was initially used for training the network.
While this approach can be seen as an artistic content generation technique, it may also improve the interpretability of the generated patterns.
We can observe that only the most dominant features are visible after the optimization.
  </p>

<!-- =================================================== -->


  <h2 id='experiment-xy2rgb'>Compositional Pattern Producing Networks as Differentiable Parameterization</h2>

<p>
  So far, we’ve explored image parameterizations that are relatively close to how we normally think of images, using pixels or Fourier components.
  In this section, we’ll explore something a bit more radical and parameterize our image as a neural network -- in particular, a Compositional Pattern Producing Network (CPPN) <d-cite key="stanley2007cppn"></d-cite>.
</p>

<p>
  CPPNs are neural networks that map $(x,y)$ positions to image colors:
</p>
<div style="margin-right: auto; margin-left: auto; margin-top: 0px; margin-bottom: 20px;">
  $(x,y) ~\xrightarrow{\tiny CPPN}~ (r,g,b)$
</div>
<p>
  By applying the CPPN to a grid of positions, one can make arbitrary resolution images.
</p>

<p>
  The parameters of the CPPN network -- the weights and biases -- determine what image is produced.
  Random parameters can produce aesthetically interesting images <d-cite key="CPPN:random"></d-cite>, but we can often produce more interesting images by learning the parameters.
  Often this is done by evolution  <d-cite key="stanley2007cppn,Nguyen2015:easilyFooled"></d-cite> but, since the network is differentiable, we can also do gradient descent.
  That is to say, CPPNs are a differentiable image parameterization --
  a general tool for parameterizing images in any neural art or visualization task.
</p>

<figure>
  <img style="width: 100%" src="images/diagrams/cppn.png"/>
  <figcaption>CPPNs are a differentiable image parameterization. We can use them for neural art or visualization tasks by backproping not only through the CNN to the image, but then past the image, through the CPPN to its parameters.</figcaption>
</figure>

<p>
  Using CPPNs as your parameterization can add an interesting artistic quality to neural art, vaguely reminiscent of light-paintings.<d-footnote>Light-painting is an artistic medium where images are created by manipulating colorful light beams with prisms and mirrors. Notable examples of this technique are the <a href="http://www.lightpaintings.com/">work of Stephen Knapp</a>.</d-footnote>
  At a more theoretical level, they can be seen as constraining the computational complexity of your images.
  When used for feature visualization, they produce distinctive images:
</p> 


<!--
These techniques have been used, for example, to demonstrate how neural networks are vulnerable to adversarial examples<d-cite key="Nguyen2015:easilyFooled"></d-cite>. (TODO: I'd prefer for avoid calling those "adversarial examples", as they don't belong to a domain the net was trained on).
-->

<figure class="base-grid">
  <div style="grid-column: screen;" id="CPPN-Examples"></div>
  <figcaption style="grid-column: text;">
    A <d-cite key="stanley2007cppn">Compositional Pattern Producing Network (CPPN)</d-cite> is used as differentiable parameterization for visualizing features at different layers<d-cite key="olah2017feature"></d-cite>. The resulting images have a novel artistic appeal as if the the original features are constructed using beams of colored light deflected by prisms and mirrors.
  </figcaption>
</figure>

  <p>
The evolution of the patterns generated by the CPPN are artistic artifacts themselves.
To maintain the metaphor of light beams and prisms, the optimization process correspond to an iterative adjustements of the beam directions and prism orientations.
Because the iterative changes have a more global effect compared to, for example, a pixel parameterization, at the beginning of the optimization only major patterns are visible.
By iteratively adjustign the weights, our immaginary prisms and beams are positioned in such a way that fine details emerge.
  </p>

  <d-figure id='CPPNAnimations'></d-figure>

   <p>
By playing with this metaphor, we can also create a new kind of animation.
Imagine that we start from the one light-painting and we move the beams and prisms to create a new one.
To achieve this result we can interpolate the weight of the CPPNs representations of the two light-paintings and generate a number of intermediate images.
As before, changes in the parameter have a global effect and create visually appealing intermediate frames.
  </p>

  <d-figure id='CPPNInterpolations'></d-figure>

  <p>
In this section we presented a parameterization that goes beyond a standard image representation.
Neural networks, a CPPN in this case, can be used to parameterize an image that is optimized for a given objective function.
More specifically, we combined a feature-visualization objective function with a CPPN parameterization to create computer generated light-paintings.
  </p>

<!-- =================================================== -->


  <h2 id='experiment-featureviz-3d'>Efficient Texture Optimization through 3D Rendering</h2>

  <p>
In previous sections we generated artistic images and animations using various image parameterization.
Would it be possible to extend this approach also for the creation of artistic 3D objects?
In this section we present a generative approach that makes use of the 3D rendering pipeline as a mean to generate artistic textures for 3D objects.
Our technique is similar to the approach that Athalye et al.<d-cite key="athalye2017synthesizing"></d-cite> used for the creation of real-world adversarial examples, as we rely on the backpropagation of the objective function to randomly sampled views of the 3D model.
We differ from existing approaches for artistic texture generation<d-cite key="kato2017neural3D"></d-cite>, as we do not modify the geometry of the object during back-propagation.
By disentangling the generation of the texture from the position of their vertices, we can create very detailed texture for complex objects.
  </p>

  <p>
Before we can describe our approach, we first need to understand how a 3D object is stored and rendered on screen. The object's geometry is usually saved as a collection of interconnected triangles called <b>triangular mesh</b> or, simply, <b>mesh</b>. To render a realistic model, a <b>texture</b> is painted over the mesh. The texture is saved as an image that is applied to the model by using the so called <b>UV-mapping</b>. Every vertex $c_i$ in the mesh is associated to a $(u_i,v_i)$ coordinate in the texture image. The model is then rendered, i.e. drawn on screen, by coloring every triangle with the region of the image that is delimited by the $(u,v)$ coordinates of its vertices.
  </p>

  <p>
You can use the following WebGL view to familiarize with these concepts. The view shows the 3D model of the famous Stanford Bunny<d-cite key="turk2005stanfordBunny"></d-cite> and the associated texture<d-cite key="levy2002least"></d-cite>. You can interact with the model by rotating, zooming and panning. Moreover, you can unfold the object to its two-dimensional texture representation. This unfolding reveals the UV mapping used to store the texture in the texture image. Note how the texture is divided in several patches that allows for a complete and undistorted coverage of the object.
  </p>

  <d-figure class="base-grid" id="BunnyModel"></d-figure>

  <p>
We aim at generating textures by optimizing for a feature-visualization objective function<d-cite key="olah2017feature"></d-cite>. A simple and straightforward way to achieve this result is to use, as texture to paint on the object, an image that is obtained by optimizing for one or more features in a layer.
However, this approach generates a texture that does not consider the underlying UV-mapping and, therefore, will create visual artifacts in the rendered object.
  </p>

<p>
  By interacting with the following model, you can identify these artifacts and compare the result with our render-based approach.
  First, seams are visible on the rendered texture, as the optimization is not aware of the underling UV-mapping and, therefore, does not optimize the texture consistently along the splitted patches of the texture.
</p>

<figure class="base-grid shaded-figure">
  <figcaption style="grid-column: kicker;">
    A Naïve approach optimizes the texture as an image with a feature visualization objective function.<br><br>
    Because this optimization is not aware of the UV mapping used to render the object, it creates artifacts such as seams, badly oriented patterns and low resolution areas.<br><br>
    You can use this model to compare the naïve approach with a render-based approach.
  </figcaption>
    <d-figure style="grid-column: text-start / screen-end; overflow: visible; contain: unset;" id="BunnyModelTextureSpaceOptimization"></d-figure>
</figure>

<p>
  Second, the generated patterns are randomly oriented on different parts of the textured object.
  In this example, the feature correspond to vertical and wigly patterns.
  Beacause the texture patches are not consistently oriented in the underlying UV-mapping, the vertical pattern is lost in the rendered model.
</p>
<p>
  Finally scaling artifacts can be seen in the rendered object, which correspond to areas where the same pattern is rendered at different scales.
  In fact, the mapping does not enforce a 1:1 correspondence between triangle areas and their mapped triangle in the texture.
  Therefore, a non uniform UV-mapping may lead to major changes in size of the rendered patterns.
</p>

  <p>
Here we adopt a different approach that overcomes this limitation by making use of the 3D rendering pipeline.
Intuitively, we optimize not directly in the texture space, but rather through the images that are generated by the 3D renderer and, therefore, are closer to what a user will see.
The following diagram presents an overview of the proposed pipeline:
  </p>

  <img style="width: 100%" src="images/diagrams/featurevis-3d.png"/>

  <p>
We start the process by randomly initializing the texture with a Fourier parameterization.
At every training iteration we sample a random camera position, which is oriented towards the center of mass of the object, and we render the textured object as an image.
We then backpropagate the gradient of the desired objective function, i.e., the feature of interest in the neural network, to the rendered image.
  </p>

  <p>
However, an update of the rendered image does not correspond to an update to the texture that we aim at optimizing. Hence, we need to further propagate the changes to the object's texture.
The propagation is easily implemented by applying a reverse UV-mapping, as for each pixel on screen we know its coordinate in the texture.
By modifying the texture, during the following optimization iterations, the rendered image will incorporate the changes applied in the previous iterations.
  </p>

<figure class="base-grid shaded-figure">
  <figcaption style="grid-column: kicker;">
    Textures are generated by optimizing for a feature visualization objective function.
    Seams in the textures are hardly visible and the patterns are correctly oriented.
  </figcaption>
    <d-figure style="grid-column: text-start / screen-end; overflow: visible; contain: unset;" id="3DFeatureVizExamples"></d-figure>
</figure>

<p>
The resulting textures are consistently optimized along the cuts, hence removing the seams and enforcing an uniform orientation for the rendered object.
Morever, since the function optimization is disentangled by the geometry of the object, the resolution of the texture can be arbitrary high.
In the next section we will se how this framework can be reused for performing an artistic style transfer to the object's texture.
</p>

<!-- =================================================== -->

  <h2 id='experiment-styletransfer-3d'>Style Transfer for Textures through 3D Rendering</h2>

  <p>
Once that we have a framework for the efficient backpropagation of the gradients in the UV-mapped texture, we can adapt existing style transfer techniques for 3D objects.
Similarly to the 2D case, we aim at redrawing the original object's texture with the style of a user-provided image.
The following diagram presents an overview of the approach:
  </p>

  <img style="width: 100%" src="images/diagrams/styletransfer-3d.png"/>

  <p>
The algorithm works in similar way to the one presented in the previous section, starting from a randomly initialized texture.
At each iteration, we sample a random view point oriented toward the center of mass of the object and we render two images of it, one with the original texture and one with the texture that we are currently optimizing.
  </p>

  <p>
Once that the images are rendered, we optimize for the style-transfer objective function introduced by Gatys et al.<d-cite key="gatys2015"></d-cite> and we map the parameterization back in the UV-mapped texture as introduced in the previous section.
The procedure is then iterated until the desired blend of content and style is obtained in the target texture.
  </p>

  <figure class="base-grid shaded-figure">
    <figcaption style="grid-column: kicker;">
      Textures are generated by optimizing for a style transfer objective function.
    </figcaption>
      <d-figure style="grid-column: text-start / screen-end; overflow: visible; contain: unset;" id="3DStyleTransferExamples"></d-figure>
  </figure>
<p>
Because every view is optimized independently, the optimization is forced to add all the style elements at every iteration.
For example, if we use as style image the Van Gogh's "Starry Night" painting, stars will be added in every single view.
We found that more pleasing results, as those presented above, are obtained by introducing a sort of "memory" of the style of previous views.
To this end, the style loss is computed as the weighted average of the loss at the current an previous iteration.
</p>

<p>
The resulting textures combine elements models of the desired style, while preserving the characteristics of the original texture.
Take as an example the model created by imposing Van Gogh's starry night as style image.
The resulting texture contains the repetitive and vigorous brush strokes that characterize Van Gogh's work.
However, despite the style image contains only cold tones, the resulting fur has a warm orange undertone as it is preserved from the original texture.
Even more interesting is how the eyes of the bunny are preserved when different styles are transfered.
For example, when the style is obtained from the Van Gogh's painting, the eyes are transformed in a star-like swirl, while if Kandinsky's work is used, they become abstract patterns that still resemble the original eyes.
</p>

<div>
  <div style="background-image: url('images/printed_bunny.jpg'); height: 400px; width: 450px; background-size: cover;
  background-position: center; width: 50%;  margin: 0 auto;margin-bottom:15px;"></div>
</div>


<p>
Our approach combines well with modern digital fabrication techniques<d-cite key="gershenfeld2012make"></d-cite>.
The geometry of the object is not part of its parameterization and, therefore, it is not modified during the optimization process.
Hence, a faithful replica of the object can be 3D printed and, by adopting the latest color printing technologies<d-footnote>Full color sandstone was used to print and color our model.</d-footnote>, it can be textured accordingly to our generated style texture.
</p>

<!-- =================================================== -->

  <h2 id='conclusions'>Conclusions</h2>
  <p>
In this article we explored several novel differentiable parameterizations that generates different kind of artistic artifacts such as images, animations and 3D objects.
However, we believe that we explored only a small fraction of the possible image parameterizations for the creation of neural artistic content and exciting new possibilities lie ahead.
  </p>



  <p>
Potential future research directions may include combining the described approaches in creative ways.
An application that comes to mind is the extension of the texture synthesis approach to material or reflectance model synthesis.
  </p>

  <p>
Another interesting direction is to go beyond differential parameterizations.
One possible approach to this is proposed by Kato et al.<d-cite key="kato2017neural3D"></d-cite> to extend optimization techniques to mesh vertex position. We also think that, by augmenting gradient optimization techniques with evolution strategies<d-cite key="wierstra2014naturalEvolutionStrategies"></d-cite>, interesting new research opportunities open for image or scene parameterization.
  </p>


</d-article>



<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam dapibus enim eget lacus accumsan consectetur. In et semper enim, dignissim malesuada neque. Nulla tristique vestibulum nibh, non aliquam nisi sodales ut.
  </p>

  <p>
Nam vitae erat tortor. Phasellus eu pulvinar lectus. Praesent facilisis accumsan augue gravida mattis. Integer neque lacus, maximus quis tincidunt sed, auctor eget est. Vestibulum sollicitudin massa sapien, ac feugiat risus feugiat sed.
  </p>

  <p>
Etiam eu imperdiet ante. Aenean suscipit nunc tellus, at placerat tellus gravida in. Phasellus faucibus magna in pulvinar pulvinar. Praesent erat nulla, dictum ornare sem id, venenatis porta augue. Phasellus scelerisque nulla semper ante hendrerit, sed feugiat velit pellentesque.
  </p>

  <h3>Author Contributions</h3>
  <p>
    TODO: List in detail who did what.
  </p>

  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
