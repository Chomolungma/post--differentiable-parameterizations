<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
</head>

<body>

<d-front-matter>
  <script type="text/json">{
  "title": "Differentiable Image Parameterizations",
  "description": "Examples of differentiable image parameterizations that allow solving previously intractable optimization tasks.",
  "password": "params",
  "authors": [
    {
      "author": "Alexander Mordvintsev",
      "authorURL": "https://znah.net/",
      "affiliation": "Google Research",
      "affiliationURL": "https://research.google.com/"
    }
  ],
  "katex": {
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
</d-front-matter>

<d-title>
  <h1>Differentiable Image Parameterizations</h1>
</d-title>

<d-article>

<!-- =================================================== -->

  <nav>
    <!-- This TOC is for development and will be removed before publication. -->
    <style>
      d-toc#toc-dev-only ul {
        padding-left: 24px;
      }
      d-toc#toc-dev-only li {
        margin-bottom: -1.7em;
      }
    </style>
    <d-toc id='toc-dev-only'></d-toc>
  </nav>

<!-- =================================================== -->

  <h2 id='introduction'>Introduction</h2>
  <p>
Deep Neural Networks are, essentially, very entangled non-linear functions<d-cite key="lecun2015deepLearning"></d-cite>. Despite being highly non-linear, they remain differentiable functions for which the computation of partial derivatives is not only possible, but very efficient on parallel architectures<d-cite key="krizhevsky2012"></d-cite>. The computation of the derivatives with respect to the parameters enables the learning of a desired function. Network's parameters, also called weights, are modified by a tiny amount along the gradient until, after a number of iterations, the output of the network and the desired function match.
  </p>

  <p>
However, other differentiable functions can be propagated, either from the output or the activations of hidden layers, without changing the parameters of the network but rather its input<d-cite key="zeiler2014"></d-cite>.
We will refer to the space in which the input is optimized as its <b>Differentiable Image Parameterizations</b>.
The most common and simple parameterization is the image pixel space, where the changes propagated troughout the network correspond to changes in the color of each pixel.
  </p>

  <p>
Depending on which function is propagated, the parameterization will map some network's latent state to a 2D raster image.
For example, DeepDream<d-cite key="mordvintsev2015inceptionism"></d-cite> and <b>feature visualization</b><d-cite key="olah2017feature"></d-cite> objective functions can be used to map in the chosen parameterization the visual patterns that the network learned to detect.
By optimizing for different objective functions, <b>artistic image style transfer</b> can be implemented.
This application was pioneered by Gatys et al. <d-cite key="gatys2015"></d-cite> and is one of the most widespread applications of neural networks for the generation of artistic contents.
The resulting parameterization creates a raster image that preserves the content of a desired image and the style of a different one.
  </p>

  <p>
In this article we explore some more exotic image parameterization and optimization functions that allows for the creation of artistic results. The proposed techniques achieve generative capabilities through an iterative process that makes small adjustments to randomly initialized parameterizations.
Hence, we do not require the creation ad-hoc feed-forward networks<d-cite key="li2017universal"></d-cite> that learn to emulate the iterative process or the implementation of generative models, such as Generative Adversarial Networks (GANs)<d-cite key="goodfellow2014:adversarial"></d-cite> or Variational Autoencoders (VAEs)<d-cite key="Diederik2013:VAE"></d-cite>.
  </p>

  <p>
We propose different parameterizations for images and extensions that allow for the creation of artistic 3D models.
We believe that our examples will inspire researchers and computational artists in devising novel and exciting computational art.
To further facilitate the development, we release the code used for generating our examples as <a href="https://colab.research.google.com/">CoLab</a> notebooks.
  </p>

<!-- =================================================== -->

  <h2 id='experiment-aligned'>Aligned Neuron Interpolation</h2>

  <p>
As an introductory example, we explore how <a href="https://distill.pub/2017/feature-visualization/#interpolation">aligned interpolation neuron visualizations</a> are created.
Throughout this article we will often optimize a randomly initialized parameterization to generate the patterns that are detected by a neurons, channels or layers in a neural network.
We refer to the result of this procedure as the optimization for a feature visualizations objective function<d-cite key="olah2017feature"></d-cite>, as it reveal the features that the network is detecting in different layers.
  </p>

  <p>
Images can be optimized not only for a single feature, but for a <a href="https://en.wikipedia.org/wiki/Convex_combination">convex combination</a> of two or more features.
By gradually changing the interpolation weight given to one neuron from the other, we can generate compelling animations that shows how a pattern non-linearly blend into another.
  </p>
  <d-figure class="base-grid" id='AlignedInterpolationExamples'></d-figure>

  <p>
The above results enforces an underlying common parameterization for all the intermediate results that lead to the creation of similar image locations for similar patterns.
If the common parameterization is not enforced, patterns would emerge at different locations in every frame.
  </p>

  <p>
To see the problem with an independent optimization, compare the following animations.
For the animation on the left, every frame is independently optimized from the others, resulting in an inconsistent visual appeal. The animation on the right is generated by optimizing each frame with a shared parameterization that we will detail shortly.
The result does not only reveal the transition from one pattern to another, but we believe it has also artistic merits.
  </p>
  <d-figure id='AlignedInterpolationAnimations'></d-figure>
  <p>
Visual alignment is obtained by combining a low-resolution shared parameterization $ P_{\text{shared}}$ and high-resolution parameterization $P_{\text{unique}}^i$ that is unique to each frame $i$.
Each individual frame $i$ of the animation is then parameterized as a combination $P^i$ of the two, $P^i = N(P_{\text{unique}}^i + P_{\text{shared}})$, where $N$ is usually the logistic function.
  </p>

  <div>
    <%= require('!svg-inline-loader!../static/images/lowres-tensor.svg') %>
  </div>

  <p>
    Intuitively, the shared parameterization provides a common reference for the displacement of major patterns, while the unique parameterizations give to each frame its own visual appeal based on its interpolation weights.
  </p>

  <p>
This first example shows a first glimpse of the creative power of custom and properly designed image parameterizations.
In a way, we may think as parameterizations as the clay that a artist can use to sculpt computational art.
  </p>

<!-- =================================================== -->

  <h2 id='experiment-styletransfer'>Good parameterizations generalize style transfer for non-VGG networks
</h2>

  <p>
Neural artistic style transfer<d-cite key="gatys2015"></d-cite> is one of the first and, by far, one of the most widespread application of generative artistic capabilities of convolutional neural networks.
Almost all iterative style transfer implementations utilize a variant of the <b>VGG neural network</b> architecture<d-cite key="simonyan2014vgg"></d-cite> while attempts at using different architectures seem to produce less visually appealing results
<d-footnote>
Examples of experiments performed by using different architectures can be find on <a href="https://medium.com/mlreview/getting-inception-architectures-to-work-with-style-transfer-767d53475bf8">Medium</a>, <a href="https://www.reddit.com/r/MachineLearning/comments/7rrrk3/d_eat_your_vggtables_or_why_does_neural_style/">Reddit</a> and <a href="https://twitter.com/hardmaru/status/954173051330904065">Twitter</a>.
</d-footnote>.
  </p>

  <p>
Various hypothesis have been made to explain the difference in performance between VGG and other models.
A possible explanation is that, being the VGG architecture bigger than others, it manages to capture more information that is not related to the task at hand and that is discarded by other models.
While this information does not give, for example, greater classification accuracy, it improves the generalization of the VGG architecture to a task like artistic style transfer.
Another possible reason is that other models perform more aggressive downsampling, in the form of max-pooling layers, compared to the VGG network.
  </p>

  <p>
Here we demonstrate that the parameterization used to represent the image is as important, if not more important, than the model chosen.
We show that artistic style transfer images produced by the GoogLeNet<d-cite key="szegedy2015googlenet"></d-cite> architecture can be as good as those produced by the VGG network if the right parameterization is chosen.
  </p>

<d-figure class="base-grid" id='StyleTransferExamples'></d-figure>

  <p>
Previous work on feature visualization<d-cite key="olah2017feature"></d-cite> showed that adopting a different parameterization works as preconditioner for the optimization, making it more robust to local minima.
Olah et al.<d-cite key="olah2017feature"></d-cite> showed that high frequency artifacts in optimization results are avoided by working in a decorrelated and whitened space and by enforcing geometric robustness
<d-footnote>
Geometric robustness is achieved by stochastically jitter, rotate and scale the image before applying the optimization step.
</d-footnote>.
  </p>
  <p>
For images, this means that the gradient descent must not be performed in the pixel space, but rather in the space of the Fourier basis where frequencies are scaled so that they all have equal energy. By adopting this parameterization, the style transfer loss function proposed by Gatys<d-cite key="gatys2015"></d-cite> produces convincing results like the ones presented above for non-VGG architectures.
  </p>

  <p>
Before we proceed in developing more exotic parameterizations, it is good to reflect on what we discussed so far.
Differentiable image parameterizations are a key aspect of optimization techniques for artistic content generation.
Underestimating the role of a properly chosen parameterization makes the difference between a poor artistic artifact and a good one.
  </p>

<!-- =================================================== -->

  <h2 id='experiment-featureviz-rgba'>Generation of Semi-Transparent Patterns</h2>
  <p>
Most of existing optimization techniques work on an input-space that has the same format as the input used to originally train the network.
For networks trained to process images, the input usually corresponds to 2d array, where each pixel is represented by RGB channel values.
In this section we investigate how to extend differentiable optimizations for channels that were not used to train the network. More specifically, we aim at creating semi-transparent images by optimizing the alpha channel of an image in combination with its RGB channels.
  </p>

  <p>
Pixels in the <b>alpha channel</b> are assumed to have values ranging from 0, when they are fully transparent, to 1 if they are completely opaque.
Our goal is to reveal one or more features detected by the network<d-cite key="olah2017feature"></d-cite> while imposing a desired amount of transparency in the generated image.
To this end, we optimize for the feature to be visualized in the RGB image multiplied by its alpha channel and, at the same time, we minimize the squared loss of the average alpha value to a desired threshold $\alpha_t$.

  </p>

<figure class="base-grid shaded-figure">
  <figcaption style="grid-column: kicker;">
    The image is parameterized as the combination of the RGB and alpha channel.
  </figcaption>
  <d-figure style="grid-column: text-start / screen-end; overflow: visible; contain: unset;" id="SemiTransparentCombination"></d-figure>
</figure>

  <p>
By multiplying the RGB channels with the alpha channel, we are de-facto imposing a black background for transparent areas.
Therefore, if the target feature detects black or dark colors, the optimization has two ways for optimizing the result.
It may either generate a dark color in the RGB channels or reduce the transparency accordingly.
  </p>
  <p>
Since we constraint the level of transparency, given by the $\alpha_t$ threshold, the optimization is incentivized to apply the latter of the two strategies.
Unfortunately, this will lead to missing colors in the RGB channels.
The same problem holds if different static colors are chosen as background.
The optimization is never incentivized to propagate the chosen color to the generated image.
  </p>

  <p>
To solve this problem, we randomly change every background pixel at each iteration of the optimization.
In this way, we force the optimization to propagate the colors to the RGB channels and do not "cheat" by making use of a static background.
  </p>
  <p>
We experimented with two different strategies for generating the background.
The first approach rely on a randomly generated background for each pixel, while the second one uses a user provided image which is randomly cropped and used as background.
For the latter, we found that the chosen image must have a diverse palette or the same problem may arise.
  </p>

 <d-figure class="base-grid" id='SemiTransparentExamples'></d-figure>

  <p>
In this section we have shown how we can extend optimization techniques beyond the input space that was initially used for training the network.
While this approach can be seen as an artistic content generation technique, it may also improve the interpretability of the generated patterns.
We can observe that only the most dominant features are visible after the optimization.
  </p>

<!-- =================================================== -->


  <h2 id='experiment-xy2rgb'>Compositional Pattern Producing Networks as Differentiable Parameterization</h2>

  <p>
In the previous sections we presented techniques that optimize the parameterizations in the pixel or in the Fourier domain.
The target function is minimized by back propagating the gradients through the network and by making tiny changes to the values of the pixels or to the basis of the Fourier domain.
  </p>

  <p>
In this section we investigate the possibility of creating a differentiable parameterizations in the form of a <d-cite key="stanley2007cppn">Compositional Pattern Producing Network (CPPN)</d-cite>.
A CPPN is a neural network that approximates a function $\mathbf{c} = f(x,y)$ that computes a color $\mathbf{c}$ given the coordinate $x$ and $y$ in a 2-dimensional space.
CPPNs generates images of arbitrary resolution by sampling the two dimensional space accordingly.
  </p>
  <p>
  </p>
  <p>
  Even though artistic images can be generated by simply randomizing the weights in the network <d-cite key="CPPN:random"></d-cite>, more advanced techniques learns the relationships between $x$, $y$ and $\mathbf{c}$ given some additional constraints.
For example, the weights can be changed using evolutionary <d-cite key="stanley2007cppn"></d-cite> or gradient ascent<d-cite key="Nguyen2015:easilyFooled"></d-cite> algorithms.

<!--
These techniques have been used, for example, to demonstrate how neural networks are vulnerable to adversarial examples<d-cite key="Nguyen2015:easilyFooled"></d-cite>. (TODO: I'd prefer for avoid calling those "adversarial examples", as they don't belong to a domain the net was trained on).
-->
  </p>

  <p>
Here we show that CPPNs works as parameterization for an optimization approach.
The gradients of a target function, for example for a feature visualization task<d-cite key="olah2017feature"></d-cite>, are not used to optimize an image but rather the function $f(x,y)$ that the CPPN computes.
  </p>

  <div>
      <img src="/tmp_images/xy2rgb_diagram.png">
  </div>

  <p>
To this end, the gradient objective function is propagated through the convolutional network to its RGB input.
From there, the gradient is further propagated through the CPPN and its weights are changed accordingly.
  </p>

  <p>
In this sense, the CPPN acts as a parameterization that adds a novel artistic angle to the feature visualization pipeline.
To a certain degree, the generation of this images correspond to what an artist would achieve by manipulating a set of prisms and mirrors that reflect and bend colorful light beams to create a light-painting<d-footnote>Notable examples of this technique are the <a href="http://www.lightpaintings.com/">Lightpaintings</a> from Stephen Knapp.</d-footnote>.

  </p>

<figure class="base-grid">
  <div style="grid-column: screen;" id="CPPN-Examples"></div>
  <figcaption style="grid-column: text;">
    A <d-cite key="stanley2007cppn">Compositional Pattern Producing Network (CPPN)</d-cite> is used as differentiable parameterization for visualizing features at different layers<d-cite key="olah2017feature"></d-cite>. The resulting images have a novel artistic appeal as if the the original features are seen through a distorting lens.
  </figcaption>
</figure>

  <p>
The evolution of the patterns generated by the CPPN are artistic artifacts themselves.
To maintain the metaphor of light beams and prisms, the optimization process correspond to an iterative adjustements of the beam directions and prism orientations.
Because the iterative changes have a more global effect compared to, for example, a pixel parameterization, at the beginning of the optimization only major patterns are visible.
By iteratively adjustign the weights, our immaginary prisms and beams are positioned in such a way that fine details emerge.
  </p>

  <d-figure id='CPPNAnimations'></d-figure>

   <p>
By playing with this metaphor, we can also create a new kind of animation.
Imagine that we start from the one light-painting and we move the beams and prisms to create a new one.
To achieve this result we can interpolate the weight of the CPPNs representations of the two light-paintings and generate a number of intermediate images.
As before, changes in the parameter have a global effect and create interesting intermediate frames.
  </p>

  <d-figure id='CPPNInterpolations'></d-figure>

  <p>
In this section we presented a parameterization that goes beyond a standard image representation.
Neural networks, a CPPN in this case, can be used to parameterize a image that is optimized for a given objective function.
More specifically, we combined a feature-visualization objective function with a CPPN parameterization to create computer generated light-paintings.
  </p>

<!-- =================================================== -->


  <h2 id='experiment-featureviz-3d'>Efficient Texture Optimization through 3D Rendering</h2>

  <p>
In previous sections we generated artistic images and animations using various image parameterization.
Would it be possible to extend this approach also for the creation of artistic 3D objects?
In this section we present a generative approach that makes use of the 3D rendering pipeline as a mean to generate artistic textures for 3D objects.
Our technique is similar to the approach that Athalye et al.<d-cite key="athalye2017synthesizing"></d-cite> used for the creation of real-world adversarial examples, as we rely on the backpropagation of the objective function to randomly sampled views of the 3D model.
We differ from existing approaches for artistic texture generation<d-cite key="kato2017neural3D"></d-cite>, as we do not modify the geometry of the object during back-propagation.
By disentangling the generation of the texture from the position of their vertices, we can create very detailed texture for complex meshes.
  </p>

  <p>
Before we can describe our approach, we first need to understand how a 3D object is stored and rendered on screen. The object's geometry is usually saved as a collection of interconnected triangles called <b>triangular mesh</b> or, simply, <b>mesh</b>. To render a realistic model, a <b>texture</b> is painted over the mesh. The texture is saved as an image that is applied to the model by using the so called <b>UV-mapping</b>. Every vertex $c_i$ in the mesh is associated to a $(u_i,v_i)$ coordinate in the texture image. The model is then rendered, i.e. drawn on screen, by coloring every triangle with the region of the image that is delimited by the $(u,v)$ coordinates of its vertices.
  </p>

  <p>
You can use the following WebGL view to familiarize with these concepts. The view shows the 3D model of the famous Stanford Bunny<d-cite key="turk2005stanfordBunny"></d-cite> and the associated texture<d-cite key="levy2002least"></d-cite>. You can interact with the model by rotating, zooming and panning. Moreover, you can unfold the object to its two-dimensional texture representation. This unfolding reveals the UV mapping used to store the texture in the texture image. Note how the texture is divided in several patches that allows for a complete and undistorted coverage of the object.
  </p>

  <d-figure class="base-grid" id="BunnyModel"></d-figure>

  <p>
We aim at generating textures by optimizing for a feature-visualization objective function<d-cite key="olah2017feature"></d-cite>. A simple and straightforward way to achieve this result is to use, as texture to paint on the object, an image that is obtained by optimizing for one or more features in a layer.
However, this approach generates a texture that does not consider the underlying UV-mapping and, therefore, will create seams in the rendered object.
  </p>

  <p>
Here we adopt a different approach that overcomes this limitation by making use of the 3D rendering pipeline.
Intuitively, we optimize not directly in the texture space, but rather through the images that are generated by the 3D renderer and, therefore, are closer to what a user will see.
The following diagram presents an overview of the proposed pipeline:
  </p>

  <div>
      <img src="/images/3d_feature.jpg">
  </div>

  <p>
We start the process by randomly initializing the texture with a Fourier parameterization.
At every training iteration we sample a random camera position, which is oriented towards the center of mass of the object, and we render the textured object as an image.
We then backpropagate the gradient of the desired objective function, i.e., the feature of interest in the neural network, to the rendered image.
  </p>

  <p>
However, an update of the rendered image does not correspond to an update to the texture that we aim at optimizing. Hence, we need to further propagate the changes to the object's texture.
The propagation is easily implemented by applying a reverse UV-mapping, as for each pixel on screen we know its coordinate in the texture.
By modifying the texture, during the following optimization iterations, the rendered image will incorporate the changes applied in the previous iterations.
  </p>

<figure class="base-grid shaded-figure">
  <figcaption style="grid-column: kicker;">
    Textures are generated by optimizing for a feature visualization objective function.
    Seams in the textures are hardly visible and the patterns are correctly oriented.
  </figcaption>
    <d-figure style="grid-column: text-start / screen-end; overflow: visible; contain: unset;" id="3DFeatureVizExamples"></d-figure>
</figure>

<p>
The resulting textures are consistently optimized along the cuts, hence removing the seams and enforcing an uniform orientation for the rendered object.
Morever, since the function optimization is disentangled by the geometry of the object, the resolution of the texture can be arbitrary high.
In the next section we will se how this framework can be reused for performing an artistic style transfer to the object's texture.
</p>

<!-- =================================================== -->

  <h2 id='experiment-featureviz-3d'>Style Transfer for Textures through 3D Rendering</h2>

  <p>
Once that we have a framework for the efficient backpropagation of the gradients in the UV-mapped texture, we can adapt existing style transfer techniques for 3D objects.
Similarly to the 2D case, we aim at redrawing the original object's texture with the style of a user-provided image.
The following diagram depicts an overview of the approach:
  </p>

  <div>
      <img src="/images/3d_style.jpg">
  </div>

  <p>
The algorithm works in similar way to the one presented in the previous section, starting from a randomly initialized texture.
At each iteration, we sample a random view point oriented toward the center of mass of the object and we render two images of it, one with the original texture and one with the texture that we are currently optimizing.
  </p>

  <p>
Once that the images are rendered, we optimize for the style-transfer objective function introduced by Gatys et al.<d-cite key="gatys2015"></d-cite> and we map the parameterization back in the UV-mapped texture as introduced in the previous section.
The procedure is then iterated until the desired blend of content and style is obtained in the target texture.
  </p>

  <figure class="base-grid shaded-figure">
    <figcaption style="grid-column: kicker;">
      Textures are generated by optimizing for a style transfer objective function.
    </figcaption>
      <d-figure style="grid-column: text-start / screen-end; overflow: visible; contain: unset;" id="3DStyleTransferExamples"></d-figure>
  </figure>
<p>
Because every view is optimized independently, the optimization is forced to add all the style elements at every iteration.
For example, if we use as style image the Van Gogh's "Starry Night" painting, stars will be added in every single view.
We found that more pleasing results, as those presented above, are obtained by introducing a sort of "memory" of the style of previous views.
To this end, the style loss is computed as the weighted average of the loss at the current an previous iteration.
(TODO: refine... maybe example?)
</p>

<!-- =================================================== -->

  <h2 id='conclusions'>Conclusions</h2>
  <p>
In this article we explored several novel differentiable parameterizations that generates different kind of artistic artifacts such as images, animations and 3D objects.
However, we believe that we explored only a small fraction of the possible image parameterizations for the creation of neural artistic content and exciting new possibilities lie ahead.
  </p>

  <p>
TODO: Images of the paintings and bunny!
  </p>

  <p>
Potential future research directions may include combining the described approaches in creative ways.
An application that comes to mind is the extension of the texture synthesis approach to material or reflectance model synthesis.
  </p>

  <p>
Another interesting direction is to go beyond differential parameterizations.
One possible approach to this is proposed by Kato et al.<d-cite key="kato2017neural3D"></d-cite> to extend optimization techniques to mesh vertex position. We also think that, by augmenting gradient optimization techniques with evolution strategies<d-cite key="wierstra2014naturalEvolutionStrategies"></d-cite>, interesting new research opportunities open for image or scene parameterization.
  </p>


</d-article>



<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam dapibus enim eget lacus accumsan consectetur. In et semper enim, dignissim malesuada neque. Nulla tristique vestibulum nibh, non aliquam nisi sodales ut.
  </p>

  <p>
Nam vitae erat tortor. Phasellus eu pulvinar lectus. Praesent facilisis accumsan augue gravida mattis. Integer neque lacus, maximus quis tincidunt sed, auctor eget est. Vestibulum sollicitudin massa sapien, ac feugiat risus feugiat sed.
  </p>

  <p>
Etiam eu imperdiet ante. Aenean suscipit nunc tellus, at placerat tellus gravida in. Phasellus faucibus magna in pulvinar pulvinar. Praesent erat nulla, dictum ornare sem id, venenatis porta augue. Phasellus scelerisque nulla semper ante hendrerit, sed feugiat velit pellentesque.
  </p>

  <h3>Author Contributions</h3>
  <p>
    TODO: List in detail who did what.
  </p>

  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
