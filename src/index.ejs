<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
</head>

<body>

<d-front-matter>
  <script type="text/json">{
  "title": "Differentiable Image Parameterizations",
  "description": "Examples of differentiable image parameterizations that allow solving previously intractable optimization tasks.",
  "password": "params",
  "authors": [
    {
      "author": "Alexander Mordvintsev",
      "authorURL": "https://znah.net/",
      "affiliation": "Google Research",
      "affiliationURL": "https://research.google.com/"
    }
  ],
  "katex": {
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
</d-front-matter>

<d-title>
  <h1>Differentiable Image Parameterizations</h1>
</d-title>

<d-article>
  
  <nav>
    <!-- This TOC is for development and will be removed before publication. -->
    <style>
      d-toc#toc-dev-only ul {
        padding-left: 24px;
      }
      d-toc#toc-dev-only li {
        margin-bottom: -1.7em;
      }
    </style>
    <d-toc id='toc-dev-only'></d-toc>
  </nav>

  <p>
    This cites our <d-cite key="olah2017feature">Feature visualization</d-cite> article.
  </p>

  <h2 id='introduction'>Introduction</h2>
  <p>
Deep Neural Networks are essentially very entangled non-linear functions. Despite being highly non-linear, they remain differentiable functions for which the computation of partial derivatives is not only possible, but highly efficient using parallel architectures such as GPUs. The computation of the derivatives with respect to the parameters enables the learning of the desired functions. The learning is usually intended as the propagation of the gradients of the target function through the network together with a corresponding change of the network parameters. However, other functions can be propagated either from the output or the activations of hidden layers towards the input [deconv]. This approach allows for an indirect representation in the input of the non-linear functions that are computed by the network. 
  </p>

  <p>
By exploring the resulting representations, which usually come in the form of images for Convolutional Neural Networks, we can get a deeper understanding of the decisions made by the network. DeepDream [] and the work from Olah et al. [feature_viz] are just few examples of the use of emergent generative capabilities that improve network interpretability.
Moreover, these techniques power one of the most widespread artistic applications of these models which is known as “artistic image style transfer”. In this application, which was pioneered by Gatys et al.[1], an image is generated so that it preserves the content of an input image and the style of a different one [footnote with an example?]. 
  </p>
  
  <p>
It is interesting to note that, for all the experiments presented in this article, we use neural networks that were trained to perform different tasks such as image classification. This observation distinguishes these networks from models that are explicitly designed to perform generative tasks such as Generative Adversarial Networks (GANs) [] or Variational Autoencoders (VAEs) [].  Contrary to generative models, we achieve generative capabilities through an iterative process that makes small adjustments to randomly initialized images. To this end, the definition of the loss function is critical to achieve different artistic results and we propose different formulations that we believe will inspire researchers and artists.
  </p>
  
  <p>
In this article we explore some of more exotic image parameterization methods and their influence to the result. We also show how flexibility of neural input space optimization allows us to expand the application of these techniques beyond 2d images. TO DO: refine when the set of experiments is clearly defined and we have a complete story
  </p>
  
  <h2 id='experiment-aligned'>Aligned Neuron Interpolation</h2>
  
  <p>
    As an introductory example, we explore how <a href="https://distill.pub/2017/feature-visualization/#interpolation">aligned interpolation neuron visualizations</a> are created.
    The motivation for a non-standard image parameterization comes from the desire to have the interpolation be visually aligned, as in the following figure originally published in <d-cite key="olah2017feature">Feature Visualization</d-cite>:
  </p>
  <d-figure class="base-grid" id='AlignedInterpolationExamples'></d-figure>
  <p>
    To see the problem with a naive approach, compare the following animations.
    In the first animation, each image was optimized independently for a convex combination of the two neurons. The resulting animation is jumpy. In the second animation we used a shared parameterization we will detail shortly. Notice how much smoother the second animation appears: (TODO: hover to play indicator, layout)
  </p>
  <d-figure id='AlignedInterpolationAnimations'></d-figure>
  <p>
    To achieve this visual alignment we optimized the frames of the second animation as a combination of a lower-resolution shared parameterization and a higher resolution parameterization unique to each frame.
    Thus the parameterization $P^i$ of each image $i$ is $ N(P_{\text{unique}}^i + P_{\text{shared}}) $ where $N$ is usually the logistic function.
  </p>
  <div>
    <%= require('!svg-inline-loader!../static/images/lowres-tensor.svg') %>
  </div>
  
  <h2 id='experiment-styletransfer'>Style Transfer on non-VGG architectures</h2>
  
  <p>
    Explanation
  </p>
  
  <d-figure class="base-grid" id='StyleTransferExamples'></d-figure>
  
  <h2 id='experiment-featureviz-rgba'>Generation of Semi-Transparent Patterns</h2>
  <p>
To the best of our knowledge, existing techniques optimize only input spaces that come in the same form as the input used to originally train the network. Hence, for convolutional neural networks, the input usually correspond to a 3-channel image. In this section we investigate how to extend differentiable generative optimizations for channels that are not used for the training of the network. More specifically, we aim at creating semi-transparent images by optimizing the alpha channel in combination with the RGB channels
  </p>

  <p>
Pixels in the <b>alpha channel</b> assumes values ranging from 0, when they are fully transparent, to 1 if they are completely opaque. 
Our goal is to enforce a certain amount of transparency in the generated image, while performing on optimization that reveals the patterns detected by a neuron, or by a convex combination of neurons. 
To this end, we optimize for RGB image multiplied by the alpha channel to visualize the feature.
At the same time, we minimize the squared loss for the divergence of the average alpha value to a desired threshold $\alpha_t$, where a value of $\alpha_t=0.25$ is used in our examples.
  </p>

<img src="/tmp_images/draft_alpha_example.png">

  <p>
As you can see in the example above, the RGB image and the alpha channel are combined to create the image that is optimized for a desired channel or layer.
  </p>
  
  <p>  
While this approach can be seen as an artistic content generation technique, it holds also results in the interpretability of neural networks. 
As a matter of fact, only the most dominant patterns detected by the desired neuron will be visible after the optimization.
  </p>

  
  <h2 id='experiment-featureviz-3d'>Efficient Texture Optimization through 3D Rendering</h2>
  
  <p>  
So far we have been focusing only on 2D images, as they are the natural domain for the most common Deep Neural Networks models. 
Would it be possible to extend the generative capabilities that we have demonstrated in the previous sections also for the creation of artistic 3D objects without building ad-hoc networks [voxel-dnn]? In this section we present a generative approach that make use of the 3D rendering pipeline combined with the generative capabilities of neural networks. A few examples of the results are shown in the picture above.
  </p>
  </p>
Before we can describe our approach, we first need to understand how a 3D object is stored and rendered on screen. The geometry of the object is usually represented as a collection of interconnected triangles which we refer as a triangular mesh or, simply, mesh. In order to generate a realistic model, we need to add a texture over the triangular mesh. The texture is saved as an image that is then applied to the model using the so called UV-mapping. Every vertex $c_i$ in the mesh is associated to a $(u_i,v_i)$ coordinate in the texture image. The model is then rendered, i.e. drawn on screen, by coloring every triangle with the region of the image that is delimited by the $(u,v)$ coordinates of its vertices.
  </p>
  </p>
You can use the following WebGL [webgl] view to familiarize with these concepts. The view shows the 3D model of the famous Stanford Bunny []. You can interact with the model by rotating, zooming and panning. Moreover, you can unfold the object to its two-dimensional texture representation. This unfolding reveals the UV mapping used to store the texture in the two dimensional image. Note how, for complex geometries, the texture is divided in several patches that allows for a complete and non-distorted coverage of the object.
  </p>
  <d-figure class="base-grid" id="3DStyleTransferExamples"></d-figure>
    </p>
Our goal is to generate a texture by visualizing a combination of features learned by a network trained for an object recognition task [featureviz]. A simple and straightforward way to achieve this result is to use, as texture to color the object, an image that is obtained by optimizing for one or more features in a layer [feature-viz]. The resulting image is then used as texture for the object.
</p>



  <p>
    By interacting with the model, you may have noticed that the result is not optimal and there are several problems that reduce the visual quality of the rendered object. 
First, seams are clearly visible on the rendered texture [footnote: The seem on the snout is particularly evident (maybe ad images of examples)], as the optimization is not aware of the underling UV-mapping and, therefore, does not optimize the texture consistently along the splitted patches of the texture.

  </p>
  <p>
Second, the generated patterns are randomly oriented on different parts of the textured object.
Convolutional networks, like the one used in this experiment [googlelenet], are known to have poor rotation-invariance [reference]. While this is not a problem per-se, as orientation usually matters in a object recognition problem, it does not work well if the texture patches are not consistently oriented in the underlying UV-mapping.

  </p>
  <p>
All the above mentioned problems arise from the fact that the optimization of the texture does not consider the underlying UV-mapping, which in the end is responsible for the appearance of the textured object.
Here we propose a different approach that overcomes this limitation through an optimization that is driven by the images generated by the renderer and, therefore, are closer to what a user will sees.
  </p>
  <p>
We start the process by randomly initializing the texture $T$ as before.
At every training iteration we sample a random camera position $p$, which is oriented towards the center of mass of the object, and we render the textured object as an image $R_p$.
We then backpropagate the gradient of the desired function, i.e. the feature of interest, to $R_p$.
However, an update of $R_p$  does not correspond to an update to the texture that we aim at optimizing. Hence, we need to propagate the changes to $R_p$ to the texture $T$.
The propagation is easily implemented by applying a reverse UV-mapping, as for each pixel on screen we know its coordinate in the texture $T$.
By modifying $T$, during the following optimization iterations, the rendered image $R_p$ will incorporate changes in the texture performed in the previous iterations.
  </p>  
  <p>  
Compared to existing methods [], our approach disentangle the generation of the texture from the 3D position of the mesh.
This characteristic allows for the generation of very detailed texture for very detailed meshes.
The only additional step compared to a traditional backpropagation pipeline, consists in the rendering of the image $R_p$, which is efficiently computed on the GPU using OpenGL [].
  </p>  

  <h2 id='experiment-featureviz-3d'>Style Transfer for Textures through 3D Rendering</h2>

  <p>  
Once that we have a framework for the efficient backpropagation of the gradients in the UV-mapped texture, we may adapt existing style transfer techniques for 3D objects.
In this experiment we need an existing texture for the object that we want to repaint with a different style. 
Similarly to the 2D case, we refer to  the original texture as the content, while the target style is provided as an image[footnote: few of the most used style comprises Van Gogh’s Starry Night, Kandinsky ….].
  </p> 
  
    <p>  
The algorithm works in similar way to the one presented in the previous section, starting from a randomly initialized texture $T$.
At each iteration, we sample a random view point $p$ oriented toward the center of mass of the object. Then, we render an image $C_p$ showing the object with the original texture. Similarly, we render an image $R_p$ where the object is drawn with the texture $T$. 
$C_p$ represents the content that we want to preserve in the image $R_p$, while the desired style is simply given by the style image provided as input.
Once that these three elements are computed, we optimize for the style function introduced by Gatys et al.[1] and we map the parameterization back in the UV-mapped texture as introduced in the previous section.
The procedure is then iterated until the desired blend of content and style is obtained in the target texture $T$.

  </p>   

  <h2 id='experiment-xy2rgb'>xy2rgb, a CPPN</h2>

</d-article>



<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
    TODO: Thank all the people who have contributed somehow.
  </p>

  <h3>Author Contributions</h3>
  <p>
    TODO: List in detail who did what.
  </p>

  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
