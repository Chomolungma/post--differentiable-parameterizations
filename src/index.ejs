<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
</head>

<body>

<d-front-matter>
  <script type="text/json">{
  "title": "Differentiable Image Parameterizations",
  "description": "Examples of differentiable image parameterizations that allow solving previously intractable optimization tasks.",
  "password": "params",
  "authors": [
    {
      "author": "Alexander Mordvintsev",
      "authorURL": "https://znah.net/",
      "affiliation": "Google Research",
      "affiliationURL": "https://research.google.com/"
    }
  ],
  "katex": {
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
</d-front-matter>

<d-title>
  <h1>Differentiable Image Parameterizations</h1>
</d-title>

<d-article>

<!------------------------------------------------------------>
<!------------------------------------------------------------>
  
  <nav>
    <!-- This TOC is for development and will be removed before publication. -->
    <style>
      d-toc#toc-dev-only ul {
        padding-left: 24px;
      }
      d-toc#toc-dev-only li {
        margin-bottom: -1.7em;
      }
    </style>
    <d-toc id='toc-dev-only'></d-toc>
  </nav>

<!------------------------------------------------------------>
<!------------------------------------------------------------>

  <h2 id='introduction'>Introduction</h2>
  <p>
Deep Neural Networks are, essentially, very entangled non-linear functions. Despite being highly non-linear, they remain differentiable functions for which the computation of partial derivatives is not only possible, but very efficient on parallel architectures<d-cite key="krizhevsky2012"></d-cite>. The computation of the derivatives with respect to the parameters enables the learning of a desired function. Network's parameters, also called weights, are modified by a tiny amount along the gradient until, after a number of iterations, the output of the network and the desired function match.
  </p>
  
  <p>
However, other differentiable functions can be propagated, either from the output or the activations of hidden layers, without changing the parameters of the network but rather its input<d-cite key="zeiler2014"></d-cite>. This approach allows for the creation of indirect representation of the non-linear functions computed by the network directly in the input space. We will refer to the output of these optimizations as <b>Differentiable Image Representations</b> when the input space is an image.
  </p>
  
  <p>
By exploring Differentiable Image representations we can better understand the decisions made by Convolutional Neural Networks (CNNs). DeepDream<d-cite key="mordvintsev2015inceptionism"></d-cite> and the work on Feature Visualization by Olah et al.<d-cite key="olah2017feature"></d-cite> are just few examples of the creation of Differentiable Image Representations  that improve neural networks’ interpretability.
  </p>
  
  <p>
Moreover, these techniques power one of the most widespread artistic applications of neural networks which is known as <b>artistic image style transfer</b>. In this application, which was pioneered by Gatys et al.<d-cite key="gatys2015"></d-cite>, the differentiable representation is an image that preserves the <b>content</b> of a desired image and the <b>style</b> of a different one.
  </p>
  
  <p>
In this article we explore some more exotic image parameterization and optimization functions that allows for the creation of novel and artistic differentiable representations. The proposed techniques allow for a flexible definition of the optimization, as they do not require the creation of generative models, such as Generative Adversarial Networks (GANs)<d-cite key="goodfellow2014:adversarial"></d-cite> or Variational Autoencoders (VAEs)<d-cite key="Diederik2013:VAE"></d-cite>.  
  </p>
  
  <p>
Contrary to generative models, we achieve generative capabilities through an iterative process that makes small adjustments to randomly initialized differentiable representations. To this end, the definition of the loss function is critical to achieve different artistic results and we propose different formulations that goes beyond the 2D domain and we believe will inspire researchers and computational artists in devising novel computational art.
  </p>
  
<!------------------------------------------------------------>
<!------------------------------------------------------------>

  <h2 id='experiment-aligned'>Aligned Neuron Interpolation</h2>
  
  <p>
    As an introductory example, we explore how <a href="https://distill.pub/2017/feature-visualization/#interpolation">aligned interpolation neuron visualizations</a> are created.
    The motivation for a non-standard image parameterization comes from the desire to have the interpolation be visually aligned, as in the following figure originally published in <d-cite key="olah2017feature">Feature Visualization</d-cite>:
  </p>
  <d-figure class="base-grid" id='AlignedInterpolationExamples'></d-figure>
  <p>
    To see the problem with a naive approach, compare the following animations.
    In the first animation, each image was optimized independently for a convex combination of the two neurons. The resulting animation is jumpy. In the second animation we used a shared parameterization we will detail shortly. Notice how much smoother the second animation appears: (TODO: hover to play indicator, layout)
  </p>
  <d-figure id='AlignedInterpolationAnimations'></d-figure>
  <p>
    To achieve this visual alignment we optimized the frames of the second animation as a combination of a lower-resolution shared parameterization and a higher resolution parameterization unique to each frame.
    Thus the parameterization $P^i$ of each image $i$ is $ N(P_{\text{unique}}^i + P_{\text{shared}}) $ where $N$ is usually the logistic function.
  </p>
  <div>
    <%= require('!svg-inline-loader!../static/images/lowres-tensor.svg') %>
  </div>
  
<!------------------------------------------------------------>
<!------------------------------------------------------------>

  <h2 id='experiment-styletransfer'>Style Transfer on non-VGG architectures</h2>
  
  <p>
    Explanation
  </p>
  
  <d-figure class="base-grid" id='StyleTransferExamples'></d-figure>

<!------------------------------------------------------------>
<!------------------------------------------------------------>

  
  <h2 id='experiment-featureviz-rgba'>Generation of Semi-Transparent Patterns</h2>
  <p>
To the best of our knowledge, existing optimization techniques work on input-spaces that have the same shape as the input used to originally train the network.
Hence, for convolutional neural networks, the input usually correspond to a 3-channel image. 
In this section we investigate how to extend differentiable generative optimizations for channels that are not used for the training of the network. More specifically, we aim at creating semi-transparent images by optimizing the alpha channel in combination with the RGB channels.
  </p>
  

  <p>
Pixels in the <b>alpha channel</b> assumes values ranging from 0, when they are fully transparent, to 1 if they are completely opaque. 
Our goal is to enforce a certain amount of transparency in the generated image, while performing on optimization that reveals the patterns detected by a neuron, a convex combination of neurons or a layer. 
To this end, we optimize for RGB image multiplied by the alpha channel to visualize the feature.
At the same time, we minimize the squared loss for the divergence of the average alpha value to a desired threshold $\alpha_t$, where a value of $\alpha_t=0.25$ is used in our examples.
  </p>

<img src="/tmp_images/draft_alpha_example.png">

  <p>
As you can see in the examples above, the RGB image and the alpha channel are optimized together in order to create a visualization of the desired feature.
  </p>

  <p>  
By simply multiplying the RGB channels with the alpha channel, we are de-facto imposing a black background for transparent areas.
Therefore, if the target feature contains black, or dark colors in general, the optimization has two ways for optimizing the four channels.
It may either generate a dark color in the RGB channels or reduce their transparency in the alpha channel.
Since we constraint the level of opacity, given by $\alpha_t$ threshold, the optimization is incentivized to apply the latter strategy.
Unfortunately, this will lead to missing colors in the RGB channels.
The same problem holds if different static colors are chosen as background.
The optimization is never incentivized to propagate them to the RGB channels.
  </p>

  <p>
To solve this problem, we aim at changing the background colors for every pixel pixels at each iteration.
In this way, we force the optimization to propagate the colors to the RBG channels and do not "cheat" by making use of the background.
We experimented with two different strategies for generating the background.
The first approach rely on a randomly generated background for each pixel, while the second one uses a user provided image which is randomly cropped and used as background.
For the latter, we found that the chosen image must have a diverse palette or the same problem may arises. 
As an example, compare the results that are generated using the Van Gogh’s Starry night with those created using the Monet’s Water Lilies. 
  </p>


  <img src="/tmp_images/alpha_examples.png">

  
  <p>  
In this section we shown how we can extend optimization techniques beyond the input space that was initially used for training the network.
While this approach can be seen as an artistic content generation technique, it improves also the interpretability of the generated patterns. 
As a matter of fact, only the most dominant patterns detected by the desired neuron will be visible after the optimization.

  </p>

<!------------------------------------------------------------>
<!------------------------------------------------------------>


  <h2 id='experiment-xy2rgb'>Compositional Pattern Producing Networks</h2>
  
  <p>
In the previous sections we presented techniques that optimizes the Differentiable Image Representation directly in the Pixel or in the Fourier domain.
The loss function is minimized by back propagating the gradients through the network. 
Then, small changes are applied to the values of the pixels in the Differentiable Image Representation or in the basis of the Fourier domain.
  </p>
  
  <p>
In this section we investigate the possibility of creating a differentiable representation in the form of a Compositional Pattern Producing Network (CPPN) [12].
A CPPN is a neural network that approximates a function $\mathbf{c} = f(x,y)$ that, given the coordinate $x$ and $y$ of a pixels, returns its color $\mathbf{c}$.
Since the $x$ and $y$ are provided as a continuous function, CPPNs generates images of arbitrary resolution.
  </p>
  
  <p>
  Even though artistic images can be generated by simply randomizing the weights in the network [http://blog.otoro.net/2015/06/19/neural-network-generative-art/], more advanced techniques involves learning the relationship between $x$, $y$ and $\mathbf{c}$ given some additional constraints.
For example, the weights can be changed through evolutionary algorithms [12] or gradient ascent techniques [18].
These techniques have been used, for example, to demonstrate how neural networks are vulnerable to adversarial examples [footnote: Adversarial examples are inputs to machine learning models that are intentionally designed to cause the model to make a mistake][18].
  </p>
  
  <p>
In this work we are interested applying a gradient descent approach to learn CPPNs that works as differentiable representations for visualizing the features detected by a convolutional neural network.
  </p>
  
  <img src="/tmp_images/xy2rgb_diagram.png"> 
  
  <p>
The objective function is propagated through the convolutional network to its RGB input.
From the RGB channels, the gradients are then propagated further through the CPPN.
By descending the gradient in the weights of the CPPN, we are effectively changing the function $f(x,y)$ so that its output tries to match the output of a feature visualization minimization.
  </p>
  
  <p>
In this sense, the CPPN acts as differential representation that adds a novel artistic angle to the feature visualization pipeline.
The images generated by the trained CPPN are affected by non-linear distortions that, in some way, mimic a sort of magnifying lens.
  </p>
  
  <p>
Moreover, the evolution of the patterns generated by the CPPN are artistic artifacts by themself.
For pixel and Fourier domain representations, the visualized pattern slowly fade in from a randomly initialized image.
However, with a CPPN representation, bright colors warp and merge in the output image until they converge to the final image.
  </p>
   

<!------------------------------------------------------------>
<!------------------------------------------------------------>

  
  <h2 id='experiment-featureviz-3d'>Efficient Texture Optimization through 3D Rendering</h2>
  
  <p>  
So far we have been focusing only on 2D images, as they are the natural domain for the most common Deep Neural Networks models. 
Would it be possible to extend the generative capabilities that we have demonstrated in the previous sections also for the creation of artistic 3D objects without building ad-hoc networks [voxel-dnn]? In this section we present a generative approach that make use of the 3D rendering pipeline combined with the generative capabilities of neural networks. A few examples of the results are shown in the picture above.
  </p>
  </p>
Before we can describe our approach, we first need to understand how a 3D object is stored and rendered on screen. The geometry of the object is usually represented as a collection of interconnected triangles which we refer as a triangular mesh or, simply, mesh. In order to generate a realistic model, we need to add a texture over the triangular mesh. The texture is saved as an image that is then applied to the model using the so called UV-mapping. Every vertex $c_i$ in the mesh is associated to a $(u_i,v_i)$ coordinate in the texture image. The model is then rendered, i.e. drawn on screen, by coloring every triangle with the region of the image that is delimited by the $(u,v)$ coordinates of its vertices.
  </p>
  </p>
You can use the following WebGL [webgl] view to familiarize with these concepts. The view shows the 3D model of the famous Stanford Bunny []. You can interact with the model by rotating, zooming and panning. Moreover, you can unfold the object to its two-dimensional texture representation. This unfolding reveals the UV mapping used to store the texture in the two dimensional image. Note how, for complex geometries, the texture is divided in several patches that allows for a complete and non-distorted coverage of the object.
  </p>
  <d-figure class="base-grid" id="3DStyleTransferExamples"></d-figure>
    </p>
Our goal is to generate a texture by visualizing a combination of features learned by a network trained for an object recognition task [featureviz]. A simple and straightforward way to achieve this result is to use, as texture to color the object, an image that is obtained by optimizing for one or more features in a layer [feature-viz]. The resulting image is then used as texture for the object.
</p>



  <p>
    By interacting with the model, you may have noticed that the result is not optimal and there are several problems that reduce the visual quality of the rendered object. 
First, seams are clearly visible on the rendered texture [footnote: The seem on the snout is particularly evident (maybe ad images of examples)], as the optimization is not aware of the underling UV-mapping and, therefore, does not optimize the texture consistently along the splitted patches of the texture.

  </p>
  <p>
Second, the generated patterns are randomly oriented on different parts of the textured object.
Convolutional networks, like the one used in this experiment [googlelenet], are known to have poor rotation-invariance [reference]. While this is not a problem per-se, as orientation usually matters in a object recognition problem, it does not work well if the texture patches are not consistently oriented in the underlying UV-mapping.

  </p>
  <p>
All the above mentioned problems arise from the fact that the optimization of the texture does not consider the underlying UV-mapping, which in the end is responsible for the appearance of the textured object.
Here we propose a different approach that overcomes this limitation through an optimization that is driven by the images generated by the renderer and, therefore, are closer to what a user will sees.
  </p>
  <p>
We start the process by randomly initializing the texture $T$ as before.
At every training iteration we sample a random camera position $p$, which is oriented towards the center of mass of the object, and we render the textured object as an image $R_p$.
We then backpropagate the gradient of the desired function, i.e. the feature of interest, to $R_p$.
However, an update of $R_p$  does not correspond to an update to the texture that we aim at optimizing. Hence, we need to propagate the changes to $R_p$ to the texture $T$.
The propagation is easily implemented by applying a reverse UV-mapping, as for each pixel on screen we know its coordinate in the texture $T$.
By modifying $T$, during the following optimization iterations, the rendered image $R_p$ will incorporate changes in the texture performed in the previous iterations.
  </p>  
  <p>  
Compared to existing methods [], our approach disentangle the generation of the texture from the 3D position of the mesh.
This characteristic allows for the generation of very detailed texture for very detailed meshes.
The only additional step compared to a traditional backpropagation pipeline, consists in the rendering of the image $R_p$, which is efficiently computed on the GPU using OpenGL [].
  </p>  

<!------------------------------------------------------------>
<!------------------------------------------------------------>

  <h2 id='experiment-featureviz-3d'>Style Transfer for Textures through 3D Rendering</h2>

  <p>  
Once that we have a framework for the efficient backpropagation of the gradients in the UV-mapped texture, we may adapt existing style transfer techniques for 3D objects.
In this experiment we need an existing texture for the object that we want to repaint with a different style. 
Similarly to the 2D case, we refer to  the original texture as the content, while the target style is provided as an image[footnote: few of the most used style comprises Van Gogh’s Starry Night, Kandinsky ….].
  </p> 
  
    <p>  
The algorithm works in similar way to the one presented in the previous section, starting from a randomly initialized texture $T$.
At each iteration, we sample a random view point $p$ oriented toward the center of mass of the object. Then, we render an image $C_p$ showing the object with the original texture. Similarly, we render an image $R_p$ where the object is drawn with the texture $T$. 
$C_p$ represents the content that we want to preserve in the image $R_p$, while the desired style is simply given by the style image provided as input.
Once that these three elements are computed, we optimize for the style function introduced by Gatys et al.[1] and we map the parameterization back in the UV-mapped texture as introduced in the previous section.
The procedure is then iterated until the desired blend of content and style is obtained in the target texture $T$.

  </p>   



</d-article>



<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
    TODO: Thank all the people who have contributed somehow.
  </p>

  <h3>Author Contributions</h3>
  <p>
    TODO: List in detail who did what.
  </p>

  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
